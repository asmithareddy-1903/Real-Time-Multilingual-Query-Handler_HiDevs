{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79e92d8-5285-4497-9545-d9a595476b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.13/site-packages (25.3)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.13/site-packages (1.0.9)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.13/site-packages (8.1.5)\n",
      "Requirement already satisfied: sacrebleu in /opt/anaconda3/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.13/site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/lib/python3.13/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/lib/python3.13/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.13/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.13/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.13/site-packages (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# In a notebook cell prefix with ! to run shell commands\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers sentencepiece torch langdetect ipywidgets sacrebleu\n",
    "# Optional if you want OpenAI-based responses:\n",
    "!pip install openai\n",
    "# Optional for nicer text display\n",
    "!pip install rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acf5cbe-1a3d-4f77-9b3a-b57427d82f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from langdetect import detect, DetectorFactory\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "import sacrebleu\n",
    "\n",
    "DetectorFactory.seed = 0  # make langdetect deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c576e63-6dee-45bb-9ab9-72ca8c6771a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/m2m100_418M\"  # many-to-many multilingual model\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model loaded on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5e3772-374b-4fde-b6fb-ca3d78481bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception:\n",
    "        lang = \"unknown\"\n",
    "    return lang  # returns ISO 639-1 code like 'fr', 'hi', 'es'\n",
    "\n",
    "def translate_to_english(text, src_lang=None, max_length=256):\n",
    "    \"\"\"\n",
    "    Uses M2M100 to translate input text to English.\n",
    "    If src_lang not provided, we'll try to detect it.\n",
    "    \"\"\"\n",
    "    if not src_lang:\n",
    "        src_lang = detect_language(text)\n",
    "    # tokenizer needs language codes as per model (e.g., 'fr', 'hi', 'es')\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    # force target language to English\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"en\"), max_length=max_length)\n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11decd9a-da51-4535-9312-ba9adc0d1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported tokenizer language codes (sample):\n",
      "['af', 'am', 'ar', 'ast', 'az', 'ba', 'be', 'bg', 'bn', 'br', 'bs', 'ca', 'ceb', 'cs', 'cy', 'da', 'de', 'el', 'en', 'es', 'et', 'fa', 'ff', 'fi', 'fr', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hi', 'hr', 'ht', 'hu', 'hy', 'id', 'ig', 'ilo', 'is', 'it', 'ja', 'jv', 'ka', 'kk', 'km', 'kn', 'ko', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'ml', 'mn', 'mr']\n",
      "SOURCE: Hola, ¿cómo puedo cambiar mi contraseña?\n",
      "DETECTED: es\n",
      "raw detected language: es\n",
      "mapped tokenizer code: es\n",
      "ENGLISH: How can I change my password?\n",
      "---\n",
      "SOURCE: मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\n",
      "DETECTED: hi\n",
      "raw detected language: hi\n",
      "mapped tokenizer code: hi\n",
      "ENGLISH: My order is delayed. please help.\n",
      "---\n",
      "SOURCE: Bonjour, je n'arrive pas à me connecter.\n",
      "DETECTED: fr\n",
      "raw detected language: fr\n",
      "mapped tokenizer code: fr\n",
      "ENGLISH: Hi, I can’t get connected.\n",
      "---\n",
      "SOURCE: 我无法登录我的账户。\n",
      "DETECTED: zh-cn\n",
      "raw detected language: zh-cn\n",
      "mapped tokenizer code: zh\n",
      "ENGLISH: I cannot log in to my account.\n",
      "---\n",
      "SOURCE: 這是繁體中文的測試。\n",
      "DETECTED: zh-tw\n",
      "raw detected language: zh-tw\n",
      "mapped tokenizer code: zh\n",
      "ENGLISH: This is a Chinese test.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Improved detect + translate for M2M100 with robust code-mapping\n",
    "\n",
    "# show what codes the tokenizer actually supports (run once to inspect)\n",
    "print(\"Supported tokenizer language codes (sample):\")\n",
    "print(list(tokenizer.lang_code_to_token.keys())[:60])  # show first 60 keys\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def normalize_lang_for_tokenizer(raw_lang, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert langdetect/other raw language codes (e.g. 'zh-cn', 'pt-BR') into a code\n",
    "    present in tokenizer.lang_code_to_token. Returns a valid code or None.\n",
    "    \"\"\"\n",
    "    if not raw_lang:\n",
    "        return None\n",
    "    rl = raw_lang.strip()\n",
    "    # normalize common separators and case\n",
    "    rl = rl.replace('_', '-').lower()\n",
    "\n",
    "    # direct match attempts (try as-is and upper/lower variants)\n",
    "    if rl in tokenizer.lang_code_to_token:\n",
    "        return rl\n",
    "    # sometimes tokenizer uses uppercase region parts, try variants:\n",
    "    if rl.upper() in tokenizer.lang_code_to_token:\n",
    "        return rl.upper()\n",
    "    # try replacing '-' with '_' (some tokenizers use underscores)\n",
    "    rl_unders = rl.replace('-', '_')\n",
    "    if rl_unders in tokenizer.lang_code_to_token:\n",
    "        return rl_unders\n",
    "    if rl_unders.upper() in tokenizer.lang_code_to_token:\n",
    "        return rl_unders.upper()\n",
    "\n",
    "    # fallback: try language-only code (first two letters, e.g. 'zh' from 'zh-cn')\n",
    "    lang_only = rl.split('-')[0]\n",
    "    if lang_only in tokenizer.lang_code_to_token:\n",
    "        return lang_only\n",
    "    if lang_only.upper() in tokenizer.lang_code_to_token:\n",
    "        return lang_only.upper()\n",
    "\n",
    "    # specific convenient mappings (add as needed)\n",
    "    special_map = {\n",
    "        \"zh-cn\": \"zh\",    # map Chinese (simplified) to 'zh'\n",
    "        \"zh-tw\": \"zh\",    # traditional -> also 'zh' (or change to 'zh_TW' if tokenizer expects)\n",
    "        \"zh-hans\": \"zh\",\n",
    "        \"zh-hant\": \"zh\",\n",
    "        \"pt-br\": \"pt\",    # portuguese brazil -> 'pt'\n",
    "        \"en-gb\": \"en\",    # region variants\n",
    "        \"en-us\": \"en\",\n",
    "        \"es-es\": \"es\",\n",
    "        \"es-mx\": \"es\"\n",
    "    }\n",
    "    if rl in special_map and special_map[rl] in tokenizer.lang_code_to_token:\n",
    "        return special_map[rl]\n",
    "\n",
    "    # last resort: return None so caller can choose a default\n",
    "    return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def translate_to_english(text, src_lang=None, max_length=256, debug=False):\n",
    "    \"\"\"\n",
    "    Robust translation using M2M100. Tries to normalize the source language\n",
    "    so tokenizer accepts it. Falls back to auto-detect language-only code,\n",
    "    and finally to 'en' if nothing matches (meaning the model will assume English input).\n",
    "    \"\"\"\n",
    "    if not src_lang:\n",
    "        src_lang = detect_language(text)\n",
    "    if debug:\n",
    "        print(\"raw detected language:\", src_lang)\n",
    "\n",
    "    mapped = normalize_lang_for_tokenizer(src_lang, tokenizer)\n",
    "    if debug:\n",
    "        print(\"mapped tokenizer code:\", mapped)\n",
    "\n",
    "    # If still None, attempt to pick the two-letter language or let tokenizer treat as unknown.\n",
    "    if mapped is None:\n",
    "        # try two-letter code forcibly\n",
    "        if src_lang:\n",
    "            candidate = src_lang.split('-')[0].lower()\n",
    "            if candidate in tokenizer.lang_code_to_token:\n",
    "                mapped = candidate\n",
    "                if debug:\n",
    "                    print(\"falling back to candidate:\", candidate)\n",
    "    # ultimate fallback: choose 'en' as src so model will treat input as English (not ideal)\n",
    "    if mapped is None:\n",
    "        mapped = \"en\"\n",
    "        if debug:\n",
    "            print(\"ultimate fallback to 'en' as src_lang (translation may be identity)\")\n",
    "\n",
    "    # set tokenizer source language (this triggers M2M100 special token config)\n",
    "    tokenizer.src_lang = mapped\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"en\"), max_length=max_length)\n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "# quick test examples\n",
    "examples = [\n",
    "    \"Hola, ¿cómo puedo cambiar mi contraseña?\",\n",
    "    \"मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\",\n",
    "    \"Bonjour, je n'arrive pas à me connecter.\",\n",
    "    \"我无法登录我的账户。\",         # Chinese\n",
    "    \"這是繁體中文的測試。\"           # Traditional Chinese\n",
    "]\n",
    "\n",
    "for txt in examples:\n",
    "    print(\"SOURCE:\", txt)\n",
    "    print(\"DETECTED:\", detect_language(txt))\n",
    "    print(\"ENGLISH:\", translate_to_english(txt, debug=True))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3599c7f-3a19-4cb0-813a-5ee0c0664b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : Hola, ¿cómo puedo cambiar mi contraseña?\n",
      "Language : Spanish (es)\n",
      "English  : How can I change my password?\n",
      "---\n",
      "Original : मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\n",
      "Language : Hindi (hi)\n",
      "English  : My order is delayed. please help.\n",
      "---\n",
      "Original : Bonjour, je n'arrive pas à me connecter.\n",
      "Language : French (fr)\n",
      "English  : Hi, I can’t get connected.\n",
      "---\n",
      "Original : 我无法登录我的账户。\n",
      "Language : Chinese (zh)\n",
      "English  : I cannot log in to my account.\n",
      "---\n",
      "Original : 這是繁體中文的測試。\n",
      "Language : Chinese (zh)\n",
      "English  : This is a Chinese test.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# --- Add language-name + translation wrapper ---\n",
    "# Requires: langdetect (already used), optionally langcodes or pycountry for nice names.\n",
    "# If neither langcodes nor pycountry is installed, the function will return the language code as the name.\n",
    "\n",
    "# Try to import nicer language-name libraries, fall back gracefully\n",
    "try:\n",
    "    import langcodes  # pip install langcodes\n",
    "    def get_language_name(code):\n",
    "        if not code:\n",
    "            return \"Unknown\"\n",
    "        try:\n",
    "            # langcodes.normalize_tag handles things like 'zh-cn' -> 'zh-Hans' etc.\n",
    "            tag = langcodes.normalize_tag(code)\n",
    "            name = langcodes.Language.get(tag).display_name()\n",
    "            return name.capitalize() if isinstance(name, str) else str(name)\n",
    "        except Exception:\n",
    "            return code\n",
    "except Exception:\n",
    "    try:\n",
    "        import pycountry  # pip install pycountry\n",
    "        def get_language_name(code):\n",
    "            if not code:\n",
    "                return \"Unknown\"\n",
    "            # try alpha_2 or alpha_3 then fallback to the code itself\n",
    "            c = code.split('-')[0].lower()\n",
    "            try:\n",
    "                lang = pycountry.languages.get(alpha_2=c)\n",
    "                if lang and getattr(lang, \"name\", None):\n",
    "                    return lang.name\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                lang = pycountry.languages.get(alpha_3=c)\n",
    "                if lang and getattr(lang, \"name\", None):\n",
    "                    return lang.name\n",
    "            except Exception:\n",
    "                pass\n",
    "            return code\n",
    "    except Exception:\n",
    "        # no helper libraries available — return the code as name\n",
    "        def get_language_name(code):\n",
    "            return code or \"Unknown\"\n",
    "\n",
    "# Wrapper function that returns a structured result\n",
    "def translate_with_language_label(text, detect_fn=detect_language,\n",
    "                                  normalize_fn=normalize_lang_for_tokenizer,\n",
    "                                  translator_fn=translate_to_english,\n",
    "                                  debug=False):\n",
    "    \"\"\"\n",
    "    Returns a dict:\n",
    "      {\n",
    "        \"original\": <text>,\n",
    "        \"raw_detected_code\": <raw_code_from_langdetect or None>,\n",
    "        \"tokenizer_code\": <mapped_code_used_with_tokenizer>,\n",
    "        \"language_name\": <human readable name or code>,\n",
    "        \"translation\": <english_translation>\n",
    "      }\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\n",
    "            \"original\": text,\n",
    "            \"raw_detected_code\": None,\n",
    "            \"tokenizer_code\": None,\n",
    "            \"language_name\": \"Unknown\",\n",
    "            \"translation\": \"\"\n",
    "        }\n",
    "\n",
    "    raw_code = detect_fn(text)\n",
    "    mapped_code = normalize_fn(raw_code, tokenizer)\n",
    "    # If mapping returned None, try a last-resort two-letter fallback or 'und'\n",
    "    if mapped_code is None and raw_code:\n",
    "        mapped_code = raw_code.split('-')[0].lower() if isinstance(raw_code, str) else None\n",
    "        if mapped_code not in tokenizer.lang_code_to_token:\n",
    "            mapped_code = None\n",
    "\n",
    "    # Final fallback: if still None, choose 'en' as source (model will treat input as English — not ideal)\n",
    "    if mapped_code is None:\n",
    "        mapped_code = \"en\"\n",
    "\n",
    "    lang_name = get_language_name(mapped_code)\n",
    "\n",
    "    # Do translation (use mapped_code as src_lang so tokenizer is configured)\n",
    "    translation = translator_fn(text, src_lang=mapped_code)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Raw detected: {raw_code}\")\n",
    "        print(f\"Mapped tokenizer code: {mapped_code}\")\n",
    "        print(f\"Language name: {lang_name}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "\n",
    "    return {\n",
    "        \"original\": text,\n",
    "        \"raw_detected_code\": raw_code,\n",
    "        \"tokenizer_code\": mapped_code,\n",
    "        \"language_name\": lang_name,\n",
    "        \"translation\": translation\n",
    "    }\n",
    "\n",
    "# --- Demo on your earlier examples ---\n",
    "examples = [\n",
    "    \"Hola, ¿cómo puedo cambiar mi contraseña?\",\n",
    "    \"मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\",\n",
    "    \"Bonjour, je n'arrive pas à me connecter.\",\n",
    "    \"我无法登录我的账户。\",\n",
    "    \"這是繁體中文的測試。\"\n",
    "]\n",
    "\n",
    "for txt in examples:\n",
    "    res = translate_with_language_label(txt)\n",
    "    print(f\"Original : {res['original']}\")\n",
    "    print(f\"Language : {res['language_name']} ({res['tokenizer_code']})\")\n",
    "    print(f\"English  : {res['translation']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b5be93-97a7-4b9c-92a6-40598f232370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycountry in /opt/anaconda3/lib/python3.13/site-packages (24.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82edd9ac-713c-4919-ab7b-453369fb456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : Hola, ¿cómo puedo cambiar mi contraseña?\n",
      "Language : Spanish (es)\n",
      "English  : How can I change my password?\n",
      "---\n",
      "Original : मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\n",
      "Language : Hindi (hi)\n",
      "English  : My order is delayed. please help.\n",
      "---\n",
      "Original : Bonjour, je n'arrive pas à me connecter.\n",
      "Language : French (fr)\n",
      "English  : Hi, I can’t get connected.\n",
      "---\n",
      "Original : 我无法登录我的账户。\n",
      "Language : Chinese (zh)\n",
      "English  : I cannot log in to my account.\n",
      "---\n",
      "Original : 這是繁體中文的測試。\n",
      "Language : Chinese (zh)\n",
      "English  : This is a Chinese test.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "def get_language_name(code):\n",
    "    \"\"\"\n",
    "    Convert language code like 'zh', 'fr', 'hi' into full language name.\n",
    "    Fallbacks included if pycountry doesn't recognize the code.\n",
    "    \"\"\"\n",
    "    if not code:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # If code has region (zh-cn), keep only 'zh'\n",
    "    clean_code = code.split(\"-\")[0].lower()\n",
    "\n",
    "    # Try alpha-2 match (most common)\n",
    "    lang = pycountry.languages.get(alpha_2=clean_code)\n",
    "    if lang and hasattr(lang, 'name'):\n",
    "        return lang.name\n",
    "\n",
    "    # Try alpha-3 (e.g., 'zho' for Chinese)\n",
    "    lang = pycountry.languages.get(alpha_3=clean_code)\n",
    "    if lang and hasattr(lang, 'name'):\n",
    "        return lang.name\n",
    "\n",
    "    # Manual overrides for very common languages missing from pycountry\n",
    "    manual_map = {\n",
    "        \"zh\": \"Chinese\",\n",
    "        \"jw\": \"Javanese\",\n",
    "        \"ceb\": \"Cebuano\",\n",
    "        \"ilo\": \"Ilocano\",\n",
    "    }\n",
    "    if clean_code in manual_map:\n",
    "        return manual_map[clean_code]\n",
    "\n",
    "    # Last fallback\n",
    "    return code\n",
    "\n",
    "\n",
    "def translate_with_language_label(text, detect_fn=detect_language,\n",
    "                                  normalize_fn=normalize_lang_for_tokenizer,\n",
    "                                  translator_fn=translate_to_english,\n",
    "                                  debug=False):\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\n",
    "            \"original\": text,\n",
    "            \"raw_detected_code\": None,\n",
    "            \"tokenizer_code\": None,\n",
    "            \"language_name\": \"Unknown\",\n",
    "            \"translation\": \"\"\n",
    "        }\n",
    "\n",
    "    raw_code = detect_fn(text)\n",
    "    mapped_code = normalize_fn(raw_code, tokenizer)\n",
    "\n",
    "    if mapped_code is None and raw_code:\n",
    "        mapped_code = raw_code.split(\"-\")[0].lower()\n",
    "        if mapped_code not in tokenizer.lang_code_to_token:\n",
    "            mapped_code = None\n",
    "\n",
    "    if mapped_code is None:\n",
    "        mapped_code = \"en\"\n",
    "\n",
    "    # NEW: Full language name here\n",
    "    lang_name = get_language_name(mapped_code)\n",
    "\n",
    "    translation = translator_fn(text, src_lang=mapped_code)\n",
    "\n",
    "    return {\n",
    "        \"original\": text,\n",
    "        \"raw_detected_code\": raw_code,\n",
    "        \"tokenizer_code\": mapped_code,\n",
    "        \"language_name\": lang_name,        # Full language name added\n",
    "        \"translation\": translation\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "examples = [\n",
    "    \"Hola, ¿cómo puedo cambiar mi contraseña?\",\n",
    "    \"मेरे ऑर्डर में देरी हो रही है। कृपया मदद करें।\",\n",
    "    \"Bonjour, je n'arrive pas à me connecter.\",\n",
    "    \"我无法登录我的账户。\",\n",
    "    \"這是繁體中文的測試。\"\n",
    "]\n",
    "\n",
    "for txt in examples:\n",
    "    res = translate_with_language_label(txt)\n",
    "    print(f\"Original : {res['original']}\")\n",
    "    print(f\"Language : {res['language_name']} ({res['tokenizer_code']})\")\n",
    "    print(f\"English  : {res['translation']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba68ef8-ff83-4277-a913-f309a441ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a customer message in any language:  這是繁體中文的測試。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected Language: Chinese (zh)\n",
      "English Translation: This is a Chinese test.\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter a customer message in any language: \")\n",
    "\n",
    "res = translate_with_language_label(text)\n",
    "\n",
    "print(\"\\nDetected Language:\", res['language_name'], f\"({res['tokenizer_code']})\")\n",
    "print(\"English Translation:\", res['translation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0bbeeb-da19-4ed8-8282-b6ed1c2e4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_reply(english_text):\n",
    "    text = english_text.lower()\n",
    "    if \"password\" in text or \"login\" in text:\n",
    "        return \"Please reset your password using the link or contact support.\"\n",
    "    if \"order\" in text or \"delivery\" in text:\n",
    "        return \"Your order seems delayed. Please provide your order ID.\"\n",
    "    return \"Thank you for your message. How can I assist you further?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e57aaff-2a3d-4b5b-874b-0ceec4741b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auto Reply: Thank you for your message. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "reply = simple_reply(res['translation'])\n",
    "print(\"\\nAuto Reply:\", reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f735a307-6b4e-41c7-9c2d-bc3d8cb833d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76448260874c478f884d3f051ce4faae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Message:', layout=Layout(height='120px', width='100%'), placeholder='Type or p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d731860fd5d4672adcb6d2ef9de8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Translate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e718075d6fff4550b0e2ec3470a76c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "inp = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type or paste message here...',\n",
    "    description='Message:',\n",
    "    layout=widgets.Layout(width='100%', height='120px')\n",
    ")\n",
    "\n",
    "btn = widgets.Button(description=\"Translate\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        res = translate_with_language_label(inp.value)\n",
    "        print(f\"Detected Language: {res['language_name']} ({res['tokenizer_code']})\")\n",
    "        print(f\"English Translation:\\n{res['translation']}\\n\")\n",
    "        print(\"Auto Reply:\", simple_reply(res['translation']))\n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "display(inp, btn, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed1800c0-3148-490f-9e82-981d61e0249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel OK. Model in globals? True tokenizer? True translate_with_language_label? True translate_to_english? True simple_reply? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Kernel OK. Model in globals?\", 'model' in globals(), \n",
    "      \"tokenizer?\", 'tokenizer' in globals(),\n",
    "      \"translate_with_language_label?\", 'translate_with_language_label' in globals(),\n",
    "      \"translate_to_english?\", 'translate_to_english' in globals(),\n",
    "      \"simple_reply?\", 'simple_reply' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d1bb2-47d8-4bd3-b330-30876f57878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: Hola, ¿cómo puedo cambiar mi contraseña?\n"
     ]
    }
   ],
   "source": [
    "for s in [\"Hola, ¿cómo puedo cambiar mi contraseña?\", \"我无法登录我的账户。\"]:\n",
    "    print(\"INPUT:\", s)\n",
    "    res = translate_with_language_label(s, debug=True)\n",
    "    print(\"-> Detected:\", res['language_name'], f\"({res['tokenizer_code']})\")\n",
    "    print(\"-> English:\", res['translation'])\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7dbc0-dcc9-452f-b255-994dd3de17a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
